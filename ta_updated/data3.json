{"nodes":[{"keyword":["Automatically Acquired Knowledge"],"id":["7"],"sumbu_x":"Senseval-2 fine-grained English testing corpus and SemEval 2007 Task 7 coarse-grained testing corpus. ","sumbu_y":"2012","children":[{"id":"7","judul":"Word Sense Disambiguation with Automatically Acquired Knowledge","peneliti":"Ping Chen, Wei Ding, Max Choly, Chris Bowes","tahun_publikasi":"2012","masalah":"WSD that applied in any real world applications","deskripsi_masalah":"WSD that applied in any real world applications with automatically acquired knowledge","keyword":"Automatically Acquired Knowledge","domain_data":"Senseval-2 fine-grained English testing corpus and SemEval 2007 Task 7 coarse-grained testing corpus. ","deskripsi_domain_data":"Testing with two large scale WSD evaluation corpora, Senseval-2 fine-grained English testing corpus and SemEval 2007 Task 7 coarse-grained testing corpus. ","metode":"Automatically Acquired Knowledge","deskripsi_metode":"1) Corpus building through search engines<br\/>\r\n2) Document cleaning<br\/>\r\n3) Sentence segmentation<br\/>\r\n4) Parsing<br\/>\r\n5) Dependency relation merging<br\/>\r\n6) Dependency relation normalization<br\/>","hasil":"Achieves 82.64% in both precision and recall, which clearly outperforms the best unsupervised WSD system and performs similarly as the best supervised system<br\/>\r\n\r\nOur WSD method overcomes the knowledge acquisition bottleneck faced by many current WSD systems. Our main finding is the \u201cgreater-sum\u201d disambiguation capability of these three knowledge\r\nsources,the SemEval-2007 and Senseval-2 corpora","creater":""}],"size":[1]},{"keyword":["Text Segmentation"],"id":["8"],"sumbu_x":"Test corpus","sumbu_y":"2011","children":[{"id":"8","judul":"An Efficient Linear Text Segmentation Algorithm Using Hierarchical Agglomerative Clustering","peneliti":"Ji-Wei Wu","tahun_publikasi":"2011","masalah":"Efficient linear text \r\nsegmentation","deskripsi_masalah":"Efficient linear text \r\nsegmentation algorithm based on hierarchical agglomerative \r\nclustering","keyword":"Text Segmentation","domain_data":"Test corpus","deskripsi_domain_data":"Test corpus consists of 700 samples. A \r\nsample is a concatenation of ten text segments. The 700 samples are divided into 4 sets according to the range of the number of sentences","metode":"Hierarchical learning strategy","deskripsi_metode":"Tokenization, stopword removal, \r\nand stemming. After text preprocessing, the text can be represented \r\nas vectors, each of which represents a sentence within the \r\ntext. A part of sentence similarities are then computed to \r\nconstruct the sentence-similarity matrix. Finally, the optimal \r\ntopic boundaries are identified by the proposed algorithm. ","hasil":"Linear text segmentation \r\nalgorithm (i.e., TSHAC) outperforms the linear time algorithm, TextTiling. TSHAC also provides comparable results with other algorithms. TSHAC provides a fully automatic process for linear text segmentation without auxiliary knowledge base, parameter setting, or user involvement.","creater":""}],"size":[1]},{"keyword":["Feature Selection"],"id":["10"],"sumbu_x":"Chinese text classification corpus","sumbu_y":"2005","children":[{"id":"10","judul":"A New Approach to Feature Selection in Text Classification","peneliti":"Yi Wang, Xiao-Jing Wang","tahun_publikasi":"2005","masalah":"New approach to feature selection to do feature reduction","deskripsi_masalah":"New approach to feature selection to do feature reduction, which is a constituent process in representing texts.","keyword":"Feature Selection","domain_data":"Chinese text classification corpus","deskripsi_domain_data":"Divide the corpus into two non-intersected sets: a training set containing 10 categories with 100 texts in each and a test set containing the same 10 categories with another 100 texts in each also","metode":"Variance-mean based feature filtering","deskripsi_metode":"Variance-mean based feature filtering method of feature selection to do feature reduction in the representation phase.","hasil":"Variance-mean method can gain higher performance at a very low dimension, and quickly reach a peak, which means much less computing time and almost best performance than DF, CHI.","creater":""}],"size":[1]},{"keyword":["Text Clustering","Information Extraction","Supervised, feature selection","Cross-language, Query Translation","Text Categorization","Text Classification","Text Categorizaton"],"id":["11","54","55","56","158","159","160"],"sumbu_x":"Five test data sets(CACM, MED, EXC, PEO and TOP)","sumbu_y":"2008","children":[{"id":"11","judul":"Text Clustering with Feature Selection by Using Statistical Data","peneliti":"Yanjun Li, Congnan Luo, Soon M. Chung","tahun_publikasi":"2008","masalah":"Extended the X2 term-category indepen-\r\ndence test","deskripsi_masalah":"Extended the X2 term-category independence test by introducing new statistical data that can measure whether the dependency between a term and a category is positive or negative, developed a new supervised feature selection method, named CHIR, which is based on the X2 statistic and the new term-category dependency measure.","keyword":"Text Clustering","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"TCFS","deskripsi_metode":"Text Clustering with Feature Selection (TCFS), which performs the clustering and the supervised feature selection alternately until convergence.","hasil":"CHIR consistently out-performs other three methods in terms of increasing the cohesiveness values of the clusters.","creater":""},{"id":"54","judul":"Unsupervised Relation Extraction from Web Documents","peneliti":"Kathrin Eichler, Holmer Hemsen and Guanter Neumann","tahun_publikasi":"2008","masalah":"Information extraction systems and technology","deskripsi_masalah":"Currently, IE systems are usually domain-dependent and adapting the system to a new domain requires a high amount of manual labour, such as specifying and implementing relation\u00e2\u20ac\u201cspecific extraction patterns manually or annotating large amounts of training corpora. Consequently, current IE technology is highly statically and inflexible with respect to a timely adap tation to new requirements in form of new topics.","keyword":"Information Extraction","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"Language guesser tool, LingPipe, named entity recognition, coreference resolution","deskripsi_metode":"In order to restrict the processing to sentences written in English, we apply a language guesser tool, lc4j (Lc4j, 2007) and remove sentences not classified as written in English. To all remaining sentences, we apply LingPipe (LingPipe, 2007) for sentence boundary detection, named entity recognition (NER) and coreference resolution.","hasil":"From the extracted relations, the system built 306 clusters of two or more instances, which were manually evaluated by two authors of this paper. 81 of our clusters contain two or more instances of exactly the same relation, mostly due to the same sentence appearing in several documents of the corpus. Of the remaining 225 clusters, 121 were marked as consistent (i.e., all instances in the cluster express a similar relation), 35 as partly consistent (i.e., more than half of the instances in the cluster express a similar relation), 69 as not use- ful.","creater":""},{"id":"55","judul":"Text Clustering with Feature Selection by Using Statistical Data","peneliti":"Yanjun Li, Congnan Luo, and Soon M. Chung","tahun_publikasi":"2008","masalah":"Supervised feature selection ","deskripsi_masalah":"Supervised feature selection methods using the information gain and the \u00cf\u20212 statistic can improve the clustering performance better than unsupervised methods when the class labels of documents are available for the feature selection. However, supervised feature selection methods cannot be directly applied to document clustering because usually the required class label information is not available.","keyword":"Supervised, feature selection","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"CHIR, TCFS","deskripsi_metode":"We also developed a new supervised feature selection method, named CHIR, which is based on the \u00cf\u20212 statistic and the new term category dependency measure. Unlike CHI, CHIR selects features having strong positive dependency on the categories. Furthermore, we explored CHIR in text clustering, and developed a new text clustering algorithm, named TCFS, which stands for Text Clustering with Feature Selection. Unlike the IF method, which performs text clustering and feature selection separately, TCFS integrates a supervised feature selection method, such as CHIR, into the text clustering process.","hasil":"Feature selection methods can improve the performance of text clustering as more irrelevant or redundant terms are removed. TCFS with a supervised feature selection method, such as CHIR, CHI or CC, can achieve a better F-measure than k-means with TS.","creater":""},{"id":"56","judul":"WikiTranslate: Query Translation for Cross-Lingual Information Retrieval Using Only Wikipedia ","peneliti":"Dong Nguyen, Arnold Overwijk, Claudia Hauff, Dolf R.B. Trieschnigg, Djoerd Hiemstra, and Franciska M.G. de Jong ","tahun_publikasi":"2008","masalah":"Cross-lingual information retrieval using Wikipedia","deskripsi_masalah":"The aim of this research is to explore the possibilities of Wikipedia for query translation in CLIR. The main research question of this paper is: Is Wikipedia a viable alternative to current translation resources in cross-lingual information retrieval? This raises the following sub questions: How can queries be mapped to Wikipedia concepts? and how to create a query given the Wikipedia concepts?","keyword":"Cross-language, Query Translation","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"WikiTranslate, mapping, creating query","deskripsi_metode":"This paper presents WikiTranslate, a system which performs query  translation for cross-lingual information retrieval (CLIR) using only Wikipedia to obtain translations. We treat Wikipedia articles as representations of concepts (i.e. units of knowledge). The approach used by WikiTranslate consists of two important steps: mapping the query in source language to Wikipedia concepts and creating the final query in the target language using these found concepts.","hasil":"The system achieved a performance of 67% compared to the monolingual baseline. ","creater":""},{"id":"158","judul":"Feature Selection and Feature Extract ion for Text Categorization","peneliti":"David D. Lewis ","tahun_publikasi":"2008","masalah":" Text Categorization","deskripsi_masalah":"examine the effect \r\nof feature set size on text categorization effectiveness","keyword":"Text Categorization","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"Feature Selection and Feature Extraction","deskripsi_metode":"If a syntactic parse of text is available, then features can be defined by the presence of two or more words in \r\nparticular syntactic relationships. We call such a feature a syntactic indexing phrase.","hasil":"Good categorization performance was achieved using a statistical classifier and a proportional assignment strategy","creater":""},{"id":"159","judul":"High-Performing Feature Selection for Text Classification","peneliti":"Monica Rogati,Yiming Yang","tahun_publikasi":"2008","masalah":"Text Classification","deskripsi_masalah":"Text Classification","keyword":"Text Classification","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"High-Performing Feature Selection","deskripsi_metode":"Selected four high-performing classifiers for the feature selection experiments:\r\n1. K-Nearest Neighbors (local implementation)\r\n2. Naive Bayes (Rainbow, [7])\r\n3. Rocchio (local implementation)\r\n4. Support Vector Machines (SVMLight, [3])","hasil":"The results obtained using only 3% of the available features are among the best reported, including results obtained with the full feature set.","creater":""},{"id":"160","judul":"Summarization as Feature Selection for Text\r\nCategorization","peneliti":"Aleksander Kotcz Vidya Prabakarmurthi Jugal  Kalita","tahun_publikasi":"2008","masalah":"Text Categorization","deskripsi_masalah":"the problem addressed of evaluating the effectiveness of summarization techniques for the task of document categorization.","keyword":"Text Categorizaton","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0","metode":"Summarization as Feature Selection","deskripsi_metode":"To quantify the arguments advanced in this paper, we considered a number of simple extraction-based techniques the details are given in list below. Similar heuristics-based techniques have been used for example in [16]  [5]  [17].  In all\r\ncases, a word occurring at least 3 times in the body of a document was considered a keyword, while a word occurring at\r\nleast once in the title of a story was considered a title word.","hasil":"A framework proposed for evaluating summarization methods in the context of their utility as feature selectors in automatic text categorization. Our approach is well suited for classifiers utilizing binary feature vectors, where a\r\nfeature corresponds to the presence or absence of a word in a document.","creater":""}],"size":[1,1,1,1,1,1,1]},{"keyword":["Clustering Feature Selection"],"id":["12"],"sumbu_x":"Gisette, Optdigits, covtype, hyperspectral image","sumbu_y":"2009","children":[{"id":"12","judul":"Clustering-Based Feature Selection in Semi-supervised Problems","peneliti":"Ianisse Quinz\u00e1n, Jos\u00e9 M. Sotoca, Filiberto Pla ","tahun_publikasi":"2009","masalah":"Unlabeled information can improve significant classification result","deskripsi_masalah":"Unlabeled information can improve significant classification result","keyword":"Clustering Feature Selection","domain_data":"Gisette, Optdigits, covtype, hyperspectral image","deskripsi_domain_data":"Gisette is a big data in the UCI repository, with 5000 attributes and 13500 objects, 7000 of them labelled. Optdigits problem is about the recognition of a handwritten number. The database has 5620 samples and 64 features.Covtype database, the objective is predicting forest \r\ncover type from cartographic variables, with no remotely sensed data. This database has 54 features, 581012 objects and 7 classes. A hyperspectral image called 92AV3C corresponding to a spectral image (145 x 145 pixels, 220 bands, and 17 classes).","metode":"Hybrid method (combines supervised and \r\nunsupervised measures of information)","deskripsi_metode":"A new hybrid method for semi-supervised \r\nproblem which combines supervised and unsupervised measures of information. This approach applies a strategy to obtain a feature subset through clustering techniques.","hasil":"The unsupervised information improves the accuracy and the ssfc method is adequate.\r\nOptdigits is a database where sup technique gets high-quality features for few labeled samples. Thus, in this case \r\nthe ssfc has similar performance than sup. Nevertheless when the number of labeled samples is increased, ssfc and sup become similar to supT. ","creater":""}],"size":[1]},{"keyword":["Discrete Particle Swarm","Constituent Dependencies","Background Knowledge"],"id":["13","16","17"],"sumbu_x":"GCE-2004 dataset","sumbu_y":"2010","children":[{"id":"13","judul":"A Discrete Particle Swarm Optimization Algorithm for Domain Independent Linear Text Segmentation","peneliti":"Ji-Wei Wu, Judy C.R. Tseng, Wen-Nung Tsai ","tahun_publikasi":"2010","masalah":"Improve the performance of linear text segmentation","deskripsi_masalah":"Improve the performance of linear text segmentation","keyword":"Discrete Particle Swarm","domain_data":"GCE-2004 dataset","deskripsi_domain_data":"Choi test corpus consists of 700 samples. A sample is a concatenation of ten text segments and each segment is the first in sentences of a randomly selected document from the Brown corpus.","metode":"DPSO-SEG","deskripsi_metode":"The goal of DPSO-SEG is to identify the optimal topic boundaries of the text segments in a document.  At first, the \r\nterms within each sentence are tokenized and stemmed. Then, generic stop words are removed.  After the basic \r\npreprocessing, each sentence is represented as a term-frequency vector. Then, sentence-sentence similarity \r\nbetween a pair of sentences is computed by cosine similarity. A sentence similarity matrix of the text then constructed using the sentence-sentence similarity. Finally, the optimal \r\nboundaries are created by DPSO according to the sentence similarity matrix. ","hasil":"The value of Pk is reduced sharply with fewer numbers of iterations, and smoothly after 350 iterations. It is converged at about 1500 iterations. the performance of DPSO-SEGC99 is better than DPSO-SEG. DPSO-SEGC99 also converges faster.","creater":""},{"id":"16","judul":"Exploiting Constituent Dependencies for Tree Kernel-Based Semantic Relation Extraction","peneliti":"Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian ","tahun_publikasi":"2010","masalah":"Dynamically determine the tree span for relation extraction by exploiting constituent dependencies","deskripsi_masalah":"Dynamically determine the tree span for relation extraction by exploiting constituent dependencies to integrate dependency information, which has been proven very useful to relation extraction, with the structured syntactic information to construct a concise and effective tree span specifically targeted for relation extraction. Explore interesting combined entity features for relation extraction via a unified parse and semantic tree. ","keyword":"Constituent Dependencies","domain_data":"GCE-2004 dataset","deskripsi_domain_data":"ACE RDC 2004 corpus as the benchmark data that contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes","metode":"Condense NounPhrases (NPs)\r\n","deskripsi_metode":"(1) Modification within base-NPs \r\n(2) Modification to NPs\r\n(3)Arguments\/adjuncts to verbs\r\n(4)Coordination conjunctions\r\n(5)Modification to other constituents","hasil":"the improvements of different tree setups over SPT. DSPT performs best among DSPT, SPT, CS-SPT. It also shows that the Unified Parse and Semantic Tree with Feature-Paired Tree perform significantly better than the other two tree setups (i.e., CS-SPT and DSPT).","creater":""},{"id":"17","judul":"Exploiting Background Knowledge for Relation Extraction","peneliti":"Yee Seng Chan and Dan Roth","tahun_publikasi":"2010","masalah":"Supervised RE","deskripsi_masalah":"Improve the performance of RE by considering the relationship between our relations of interest, as well as how they relate to some existing knowledge resources","keyword":"Background Knowledge","domain_data":"GCE-2004 dataset","deskripsi_domain_data":"ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments. ACE-2004 defines 7 coarse-grained relations and 23 fine-grained relations","metode":"Coarse-grained predictions","deskripsi_metode":"Using the coarse-grained predictions which should intuitively be more reliable, to improve the fine-grained predictions.Using Novel to contrain the predictions of the fine-grained.","hasil":"Performing the usual evaluation on mentions gives similar performance figures. All the background knowledge helped to improve performance, providing a total improvement of 3.9 to our basic RE system. Improves the performance of coarse-grained relation predictions.","creater":""}],"size":[1,2]},{"keyword":["First Order Statistics"],"id":["14"],"sumbu_x":"DNA microarray","sumbu_y":"2012","children":[{"id":"14","judul":"First Order Statistics Based Feature Selection: A Diverse and Powerful Family of Feature Seleciton Techniques","peneliti":"Taghi Khoshgoftaar, David Dittman, Randall Wald, and Alireza Fazelpour","tahun_publikasi":"2012","masalah":"First Order Statistics (FOS) based feature selection","deskripsi_masalah":"First Order Statistics (FOS) based feature selection using seven related univariate\r\nfeature selection metrics","keyword":"First Order Statistics","domain_data":"DNA microarray","deskripsi_domain_data":"The datasets are all DNA microarray datasets acquired from a number of different real world bioinformatics, genetics, and medical projects. Use datasets with two classes for example:\r\ncancerous\/non-cancerous or relapse\/no relapse). ","metode":"Datasets, feature subset size, similarity measure, and classification","deskripsi_metode":"Datasets, feature subset size, similarity measure, and classification","hasil":"Twenty one possible pairwise comparisons only one combination is above a 0.7 similarity across all twelve feature subset sizes: Fold Change Difference and SAM. Outside of this pair only four other pairs (S2N and Welch T Statistic, Signal to Noise and SAM, Fold Change Difference and Fisher Score, and Welch T Statistic and SAM) achieve a similarity score above 0.7 and only the combination of Welch T Statistic and Fisher Score achieves this below a feature subset size of 500","creater":""}],"size":[1]},{"keyword":["Relation Extraction"],"id":["15"],"sumbu_x":"Synonym dictionary for genes\/proteins","sumbu_y":"2007","children":[{"id":"15","judul":"Relation extraction using dependency parse trees","peneliti":"Katrin Fundel, Robert Ku\u00a8ffner, Ralf Zimmer","tahun_publikasi":"2007","masalah":"Relation extraction from free text","deskripsi_masalah":"The use of dependency parse trees as a means for biomedical relation extraction from free text. It is based on natural language preprocessing producing dependency parse trees and applying a small number of simple rules to these trees. ","keyword":"Relation Extraction","domain_data":"Synonym dictionary for genes\/proteins","deskripsi_domain_data":"Synonym dictionary for genes\/proteins, a training set (55 sentences and 103 interactions) and a test set (80 sentences and 54 interactions).","metode":"Effector-relation-effectee, relation-of-effectee-by-effector, relation-between-effector-and-effectee","deskripsi_metode":"(1) effector-relation-effectee (\u2018A activates B\u2019)\r\n(2) relation-of-effectee-by-effector (\u2018Activation of A by B\u2019)\r\n(3) relation-between-effector-and-effectee (\u2018Interaction between A\r\nand B\u2019).","hasil":"HPRD, even though being a very large\r\nand valuable source for protein interaction data, currently covers\r\nonly a small part of the human protein-protein relations from very limited relation categories. RelEx provides complementary information.","creater":""}],"size":[1]},{"keyword":["Automatic Evaluation"],"id":["18"],"sumbu_x":"ReVerb and SONEX","sumbu_y":"2012","children":[{"id":"18","judul":"Automatic Evaluation of Relation Extraction Systems on Large-scale","peneliti":"Mirko Bronzi, Zhaochen Guo, Filipe Mesquita","tahun_publikasi":"2012","masalah":"Framework for large-scale evaluation of relation extraction systems","deskripsi_masalah":"Framework for large-scale\r\nevaluation of relation extraction systems based on an automatic annotator that uses a public online database and a large web corpus.","keyword":"Automatic Evaluation","domain_data":"ReVerb and SONEX","deskripsi_domain_data":"Compare two open RE systems: ReVerb and SONEX. The input corpus for this comparison is the New York Times corpus, composed by 1.8 million documents. ReVerb  extracts relational phrases using rules over part-of-speech tags and noun-phrase chunks.","metode":"Automatic annotator","deskripsi_metode":"Use of an automatic annotator: a system capable of verifying whether or not a fact was correctly extracted. This is done by leveraging external sources of data and text, which are not available to the systems being evaluated","hasil":"About 63 million facts in G', the superset of the ground truth G. ","creater":""}],"size":[1]},{"keyword":["Partially Supervised Relation Extraction"],"id":["19"],"sumbu_x":"Three relations extracted","sumbu_y":"1992","children":[{"id":"19","judul":"Confidence Estimation Methods for Partially Supervised Relation Extraction","peneliti":"Eugene Agichtein","tahun_publikasi":"1992","masalah":"Extract structured relations between named entities","deskripsi_masalah":"Extract structured relations between named entities (e.g., a company name, a location name, or a name of a drug or a disease) from unstructured documents with minimal human effort. ","keyword":"Partially Supervised Relation Extraction","domain_data":"Three relations extracted","deskripsi_domain_data":"Three relations extracted from a collection of 145,000 articles from the New York Times from 1996, available as part of the North American News Text Corpus1.","metode":"Expectation Maximization (EM)","deskripsi_metode":"Expectation Maximization (EM) algorithms for estimating pattern and tuple confidence.","hasil":"The EM-based methods have higher accuracy than the constraint-based method","creater":""}],"size":[1]},{"keyword":["Text Segmentation, Subtopical Structure","DNA microarray datasets","Cross-language, Query Translation","Cross-language, Query Translation","Cross-language Information Retrieval, Machine Translation, Context"],"id":["67","68","69","70","71"],"sumbu_x":"Cross-linked Information Resources (CLIR)","sumbu_y":"2012","children":[{"id":"67","judul":"TopicTiling: A Text Segmentation Algorithm based on LDA","peneliti":"Martin Riedl and Chris Biemann","tahun_publikasi":"2012","masalah":"Text Segmentation","deskripsi_masalah":"The task tackled in this paper is Text Segmentation (TS), which is to be understood as the segmentation of texts into topically similar units. The challenge for a text segmentation algorithm is to find the subtopical structure of a text.","keyword":"Text Segmentation, Subtopical Structure","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"TextTiling, Latent Dirichlet Allocation","deskripsi_metode":"This algorithm is based on the well-known TextTiling algorithm, and segments documents using the Latent Dirichlet Allocation (LDA) topic model. TopicTiling uses topic IDs, obtained by the LDA inference method, instead of words. We denote this most probable topic ID as the mode (most frequent across all inference steps) of the topic assignment. These IDs are used to calculate the cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID.","hasil":"We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show significant improvements over state of the art segmentation algorithms on two standard datasets. As an additional benefit, TopicTiling performs the segmentation in lin- ear time and thus is computationally less expensive than other LDA-based segmentation methods.","creater":""},{"id":"68","judul":"First Order Statistics Based Feature Selection: A Diverse and Powerful Family of Feature Seleciton Techniques","peneliti":"Taghi Khoshgoftaar, David Dittman, Randall Wald, and Alireza Fazelpour","tahun_publikasi":"2012","masalah":"Reduction of the dimensionality of a dataset","deskripsi_masalah":"One of the most prevalent problems in DNA microarray datasets is the large degree of high dimensionality that is inherent in the data. Feature selection refers to a diverse series of techniques from the field of data mining designed for the reduction of the dimensionality of a dataset. However, there are a number of feature selection techniques which are computationally infeasible due to the severe level of high dimensionality found in DNA microarray datasets.","keyword":"DNA microarray datasets","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"First Order Statistics (FOS)","deskripsi_metode":"In order to examine the properties of these seven techniques we performed a series of similarity and classification experiments on eleven DNA microarray datasets. This paper presents a set of seven univariate feature selection techniques which we have combined into a family of techniques we name First Order Statistics (FOS) based feature selection.","hasil":"In terms of similarity, we find that each ranker in the FOS family of techniques will create diverse feature subsets when compared to other members of the family","creater":""},{"id":"69","judul":"Accurate Query Translation for Japanese-English Cross-language Information Retrieval","peneliti":"Vitaly Klyuev and Yannis Haralambous","tahun_publikasi":"2012","masalah":"Queries translation","deskripsi_masalah":"Cross-Language Information Retrieval (CLIR) can  be used to retrieve documents in one language in response to a query given in another. The usual approach consists of two steps: 1) translation of the user query into the target language and then 2) retrieval of documents in this language by using a conventional mono-lingual information retrieval system. In this paper, we propose a novel approach to translate queries for a Japanese-English CLIR task.","keyword":"Cross-language, Query Translation","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"EWC semantic relatedness, Wikipedia-based Explicit Semantic Analysis measure","deskripsi_metode":"To get all possible English senses for every Japanese term, the online dictionary SPACEALC is utilized. The EWC semantic relatedness measure is used to select the most related meanings for the results of translation. This measure combines the Wikipedia-based Explicit Semantic Analysis measure, the WordNet path measure and the mixed collocation index.","hasil":"Retrieval performance with queries generated utilizing Mecab was very low. Our preliminary experiments showed the superiority of the longest much technique applying SPACEALC over Mecab: Segmentation of Japanese texts is much more accurate.","creater":""},{"id":"70","judul":"Query Translation Using Concepts Similarity based on Quran Ontology for Cross-language Information Retrieval","peneliti":"Zulaini Yahya, Muhamad Taufik Abdullah, Azreen Azman and Rabiah Abdul Kadir ","tahun_publikasi":"2012","masalah":"Multi meaning words","deskripsi_masalah":"In Cross-Language Information Retrieval (CLIR) process, the translation effects have a direct impact on the accuracy of follow-up retrieval results. In dictionary-based approach, we are dealing with the words that have more than one meaning which can decrease the retrieval performance if the query translation return an incorrect translations.","keyword":"Cross-language, Query Translation","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"Domain ontology using Quran concepts","deskripsi_metode":"In this study we proposed a Cross-Language Information Retrieval (CLIR) method based on domain ontology using Quran concepts for disambiguating translation of the query and to improve the dictionary-based query translation.","hasil":"The experimental result shows that our proposed method brings significant improvement in retrieval accuracy for English document collections, but deficient for Malay document collections.","creater":""},{"id":"71","judul":"Combining Statistical Translation Techniques for Cross-Language Information Retrieval","peneliti":"Ferhan Ture1, Jimmy Lin, Douglas W. Oard","tahun_publikasi":"2012","masalah":"The use of old model of statistical machine translation systems","deskripsi_masalah":"Cross-language information retrieval today is dominated by techniques that rely principally on context-independent token-to-token mappings despite the fact that state-of-the-art statistical machine translation systems now have far richer translation models available in their internal representations.","keyword":"Cross-language Information Retrieval, Machine Translation, Context","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"Three types of statistical translation models","deskripsi_metode":"This paper explores combination-of-evidence techniques using three types of statistical translation models: context-independent token translation, token translation using phrase-dependent contexts, and token translation using sentence-dependent contexts. Context-independent translation is performed using statistically-aligned tokens in parallel text, phrase-dependent translation is performed using aligned statistical phrases, and sentence-dependent translation is performed using those same aligned phrases together with an n-gram language model.","hasil":"Experiments on retrieval of Arabic, Chinese, and French documents using English queries show that no one technique is optimal for all queries, but that statistically significant improvements in mean average precision over strong baselines can be achieved by combining translation evidence from all three techniques.","creater":""}],"size":[3,1,1]},{"keyword":["Classifier Chains ","Text categorization","Multi-Document Summarisation, Relation Extraction","Kernel-based, Relation Extraction","Distant supervision","Unsupervised Relation Extraction"],"id":["72","157","168","170","174","175"],"sumbu_x":"Automatic Content Extraction (ACE) corpus","sumbu_y":"2009","children":[{"id":"72","judul":"Classifier Chains for Multi-label Classification","peneliti":"Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank\t","tahun_publikasi":"2009","masalah":"Multi-label Classification","deskripsi_masalah":"The widely known binary relevance method for multi-label The widely known binary relevance method for multi-label\r\n\t\tclassification, which considers each label as an independent binary problem","keyword":"Classifier Chains ","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"novel chaining method that can model label correlations","deskripsi_metode":"exemplify this with a novel chaining method that can model label correlations while\r\n\t\tmaintaining acceptable computational complexity","hasil":"binary relevance-based methods have much to offer, especially in terms of scalability to large datasets","creater":""},{"id":"157","judul":"A Framework of Feature Selection Methods for Text Categorization","peneliti":"Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1","tahun_publikasi":"2009","masalah":" Text Categorization","deskripsi_masalah":"In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate","keyword":"Text categorization","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"A Framework of Feature Selection Methods","deskripsi_metode":"feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate.","hasil":"this new method is robust across different tasks and numbers of selected features","creater":""},{"id":"168","judul":"Multi-Document Summarisation Using Generic Relation Extraction","peneliti":"Ben Hachey","tahun_publikasi":"2009","masalah":"shallow features often break down","deskripsi_masalah":"The problem is that these shallow features often break down where underlying linguistic content needs to be compared rather than just surface structure.","keyword":"Multi-Document Summarisation, Relation Extraction","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"Generic Relation Extraction (GRE)","deskripsi_metode":"A novel representation is introduced based on generic relation extraction (GRE), which\r\naims to build systems for relation iden-\r\ntification and characterisation that can be\r\ntransferred across domains and tasks with-\r\nout modification of model parameters","hasil":"Performance for the relation representation is significantly better than anon-trivial tf*idf baseline across the range of summary lengths explored. Performance is also at\r\nleast as good as a comparable but less general representation based on event extraction. Correlation analysis suggests that different representations are\r\ncomplementary due to the fact that they perform well on different document sets.","creater":""},{"id":"170","judul":"Kernel-based Relation Extraction from Investigative Data","peneliti":"Cristina Giannone,Roberto Basili,Chiara Del Vescovo,Paolo Naggar,Alessandro Moschitti  ","tahun_publikasi":"2009","masalah":"Kernel-based Relation Extraction","deskripsi_masalah":" The recognition and storage of complex relations among subjects mentioned in these sources is a very difficult and time consuming task, ultimately based on pools\r\nof experts. ","keyword":"Kernel-based, Relation Extraction","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"SVMs","deskripsi_metode":"SVMs here are employed to produce a set\r\nof possible interpretations for domain relevant concepts","hasil":"The empirical investigation presented here shows that accurate results, comparable to the expert teams, can be achieved, and parametrization allows\r\nto fine tune the system behavior for fitting the specific domain requirements.","creater":""},{"id":"174","judul":"Distant supervision for relation extraction without labeled data","peneliti":"Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky","tahun_publikasi":"2009","masalah":" Relation extraction without labeled data","deskripsi_masalah":"Modern models of relation extraction for tasks like\r\nACE are based on supervised learning of relations\r\nfrom small hand-labeled corpora. We investigate an\r\nalternative paradigm that does not require labeled\r\ncorpora, avoiding the domain dependence of ACE-\r\nstyle algorithms, and allowing the use of corpora\r\nof any size","keyword":"Distant supervision","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"Distant supervision","deskripsi_metode":"For each pair of enti-\r\nties that appears in some Freebase relation, we find\r\nall sentences containing those entities in a large un-\r\nlabeled corpus and extract textual features to train\r\na relation classifier. ","hasil":"Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze\r\nfeature performance, showing that syntactic parse\r\nfeatures are particularly helpful for relations that are\r\nambiguous or lexically distant in their expression.","creater":""},{"id":"175","judul":"Unsupervised Relation Extraction by Massive Clustering","peneliti":"Edgar Gonza`lez, Jordi Turmo","tahun_publikasi":"2009","masalah":" Information Extraction","deskripsi_masalah":"The goal of Information Extraction is to automatically generate structured pieces of information from the\r\nrelevant information contained in text documents.","keyword":"Unsupervised Relation Extraction","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"As learning corpus we used the year 2000 subset of the\\r\\nAssociated Press section of the AQUAINT Corpus.","deskripsi_metode":"an unsupervised approach to\r\nlearning for Relation Detection, based on the use of massive\r\nclustering ensembles.","hasil":"The results obtained on the ACE Relation Mention Detection\r\ntask outperform in terms of F1 score by 5 points the state of the\r\nart of unsupervised techniques for this evaluation framework,\r\nin addition to being simpler and more flexible.","creater":""}],"size":[1,4,1]},{"keyword":["Lexical Features"],"id":["151"],"sumbu_x":"Dutch data set","sumbu_y":"2009","children":[{"id":"151","judul":"On the Role of Lexical Features in Sequence Labeling","peneliti":"Yoav Goldberg\u00e2\u02c6\u2014 and Michael Elhadad","tahun_publikasi":"2009","masalah":"Sequence Labeling","deskripsi_masalah":"Common NLP tasks, such as Named Entity Recognition and Chunking, involve the identification of spans of words belonging to the same phrase","keyword":"Lexical Features","domain_data":"Dutch data set","deskripsi_domain_data":"Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002)","metode":"On the Role of Lexical Features","deskripsi_metode":"use the technique of SVM anchoring to demonstrate that lexical features extracted\r\n\t\tfrom a training corpus are not necessary to obtain state of the art results on tasks such\r\n\t\tas Named Entity Recognition and Chunking","hasil":"that exact word forms aren't necessary for accurate classification","creater":""}],"size":[1]},{"keyword":["Combining Lexical"],"id":["152"],"sumbu_x":"grammatical features' role in English","sumbu_y":"2007","children":[{"id":"152","judul":"Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts","peneliti":"Michael J. Heilman, Kevyn Collins-Thompson, Jamie Callan, Maxine Eskenazi ","tahun_publikasi":"2007","masalah":"Improve Readability Measures for First and Second Language Texts","deskripsi_masalah":"Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts","keyword":"Combining Lexical","domain_data":"grammatical features' role in English","deskripsi_domain_data":" The first corpus was from a set of texts \r\ngathered from the Web for a prior evaluation of the language modeling approach.  The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach.","metode":"Combining Lexical and Grammatical Features","deskripsi_metode":"The statistical model used for this study is based on a variation of the multinomial Naive Bayes classifier.","hasil":"grammatical features can play a role in predicting reading difficulty levels for both first and second language texts in English","creater":""}],"size":[1]},{"keyword":["Document segmentation, Clinical NLP, Text classification"],"id":["153"],"sumbu_x":"three datasets composed of discharge summeries and radiology reports","sumbu_y":"2001","children":[{"id":"153","judul":"Statistical Section Segmentation in Free-Text Clinical Records","peneliti":"Michael Tepper1, Daniel Capurro2, Fei Xia1,2, Lucy Vanderwende3,2, Meliha Yetisgen-Yildiz2,1","tahun_publikasi":"2001","masalah":"an approach to automatic section segmentation of clinical records","deskripsi_masalah":"an approach to automatic section segmentation of clinical records such as hospital discharge summaries and radiology reports, along with section classification into pre-defined section categories.","keyword":"Document segmentation, Clinical NLP, Text classification","domain_data":"three datasets composed of discharge summeries and radiology reports","deskripsi_domain_data":"three datasets composed of discharge summaries and radiology reports to develop our statistical section segmenter and test its performance","metode":"classify each line in a document","deskripsi_metode":"classify each line in a document to indicate its membership to a section","hasil":"For Datasets 1 and 2, the performance results show that the two-step approach (Experiments 2, 3, and 4) outper forms the one-step approach (Experiment 1). For Dataset 3, the two-step approach slightly decreased the performance; however, the\r\ndifferences are too small to draw strong conclusions","creater":""}],"size":[1]},{"keyword":["Shallow Semantics"],"id":["154"],"sumbu_x":"Automatic Content Extraction (ACE) corpus","sumbu_y":"2005","children":[{"id":"154","judul":"Shallow Semantics for Relation Extraction","peneliti":"Sanda Harabagiu, Cosmin Adrian Bejan and Paul Mora\u00cb\u02dcrescu","tahun_publikasi":"2005","masalah":"Relation Extraction\t","deskripsi_masalah":"extracting meaningful relations from unstructured natural language sources","keyword":"Shallow Semantics","domain_data":"Automatic Content Extraction (ACE) corpus","deskripsi_domain_data":"Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)","metode":"Shallow Semantics","deskripsi_metode":"The method is based on information made available by shallow semantic parsers.\r\nSemantic information was used (1) to enhance a dependency tree kernel; and (2) to build semantic dependency structures used for enhanced relation extraction for several semantic classifiers","hasil":"frame semantics produce an enhancement of\r\n53.24% over previous state-of-art results in relation extraction. Furthermore, they show that semantic representations such as frames or predicate-argument structures have a wider impact on classification performance than the classification technique","creater":""}],"size":[1]},{"keyword":["Relation Extraction"],"id":["155"],"sumbu_x":"English news articles corpus","sumbu_y":"2012","children":[{"id":"155","judul":"Pattern Learning for Relation Extraction with a Hierarchical Topic Model","peneliti":"Enrique Alfonseca Katja Filippova Jean-Yves Delort","tahun_publikasi":"2012","masalah":"Relation Extraction with a Hierarchical Topic Model","deskripsi_masalah":"The main contribution of this work is presenting a variant of distance supervision for relation extraction \r\nwhere we do not use heuristics in the selection of the training data","keyword":"Relation Extraction","domain_data":"English news articles corpus","deskripsi_domain_data":"Text corpus used contains 33 million English news articles","metode":"Pattern Learning","deskripsi_metode":"Similar to other distant supervision methods, our approach takes as input an existing knowledge base containing entities and relations, and a textual corpus. In this work it is not necessary for the corpus to be related to the knowledge base. In what follows we assume that all the relations studied are binary and hold between exactly two entities in the knowledge base. We also assume a dependency parser is available, and that the entities have been automatically disambiguated using the knowledge base as sense inventory.","hasil":"The MLE base lines (in red with syntactic patterns and green with intertext) perform consistently worse than the models learned using the topic models (in pink and blue). The difference in precision, aggregated across all relations, is statistically significant at 95% confidence for most of the thresholds.","creater":""}],"size":[1]}],"links":[{"source":"1","target":"2","value":1},{"source":"2","target":"1","value":1},{"source":"2","target":"6","value":1},{"source":"2","target":"20","value":1},{"source":"10","target":"11","value":1},{"source":"12","target":"4","value":1},{"source":"11","target":"10","value":1},{"source":"2","target":"11","value":1},{"source":"8","target":"12","value":1}]}