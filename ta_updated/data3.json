{"nodes":[{"keyword":["Interactive Retrieval"],"id":["2"],"sumbu_x":"TREC ad-hoc queries, relevance judgments","sumbu_y":"2004","children":[{"id":"2","judul":"Improving Interactive Retrieval by Combining Ranked Lists and Clustering","peneliti":"Anton Leuski, James Allan","tahun_publikasi":"2004","masalah":"Organizing the documents","deskripsi_masalah":"Organizing the documents returned by an information retrieval system in response to a natural language query. ","keyword":"Interactive Retrieval","domain_data":"TREC ad-hoc queries, relevance judgments","deskripsi_domain_data":"TREC ad-hoc queries that ran against the documents in TREC volumes 2 and 4 (2.1GB) that include articles from Wall Street Journal, Financial Times, and Federal Register.<br\/> \r\nRelevance judgments supplied by NIST accessors (Harman & Voorhees, 1997)","metode":"Ranked list, clustering of the results","deskripsi_metode":"Ranked list, clustering of the results.\r\nevaluate by comparing to the ranked list and interactive relevance feedback search strategies.","hasil":"Significantly exceeds the initial performance of the ranked listand rivals in its effectiveness the traditional relevance feedback methods.","creater":""}],"size":[1]},{"keyword":["Generate surveys"],"id":["3"],"sumbu_x":"ACL Anthology","sumbu_y":"2009","children":[{"id":"3","judul":"Using Citations to Generate surveys of Scientific Paradigms","peneliti":"Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan,Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, David Zajic","tahun_publikasi":"2009","masalah":"Automatically generated survey","deskripsi_masalah":"First steps in producing an automatically generated, readily consumable, technical survey.","keyword":"Generate surveys","domain_data":"ACL Anthology","deskripsi_domain_data":"The ACL Anthology is a collection of papers.\r\n<br\/>\r\nEvaluation experiments are on a set of papers in the research area of Question Answering (QA)and another set of papers on Dependency parsing(DP).","metode":"Bibliometric lexical link mining and summarization techniques","deskripsi_metode":"Combining bibliometric lexical link mining and summarization techniques<br\/>\r\nFour summarization systems for survey-creation approach: Trimmer,LexRank, C-LexRank, and C-RR.<br\/>\r\nFor this we evaluated using : nugget-based pyramid evaluation and ROUGE","hasil":"Both citation texts and abstracts have unique survey-worthy information. Multidocument summarization especially technical survey creation\u2014benefits considerably from citation texts.","creater":""}],"size":[1]},{"keyword":["Graph Search and Matching"],"id":["4"],"sumbu_x":"Four queries from the TREC (Harman 1994)","sumbu_y":"1997","children":[{"id":"4","judul":"Multi-document Summarization by Graph Search and Matching","peneliti":"Inderjeet Mani, Eric Bloedorn","tahun_publikasi":"1997","masalah":"Summarize multiple documents","deskripsi_masalah":"Summarize the similarities and differences in information content among multiple documents in a way that is sensitive to the needs of the user.","keyword":"Graph Search and Matching","domain_data":"Four queries from the TREC (Harman 1994)","deskripsi_domain_data":"Four queries from the TREC (Harman 1994) collection of topics, with the idea of exploiting their associated (binary) relevance judgments.\r\n<BR\/>\r\nFor this experiment, 15 pairs of articles on international events were selected from searches on the World Wide Web.","metode":"Graph representation","deskripsi_metode":"Graph representation by making abstract content representation based on explicitly representing entities and the relations between entities, of the sort that can be robustly extracted by current information extraction systems. ","hasil":"Summaries were used, the performance was faster than with fulltext (F=32.36, p < 0.05, using analysis of variance F-test) without significant loss of accuracy. <br\/>\r\nShorter texts are effective enough to support accurate retrieval.<br\/>\r\nThe biggest improvement comes from the differences found using spreading","creater":""}],"size":[1]},{"keyword":["Text Segmentation"],"id":["8"],"sumbu_x":"Test corpus","sumbu_y":"2011","children":[{"id":"8","judul":"An Efficient Linear Text Segmentation Algorithm Using Hierarchical Agglomerative Clustering","peneliti":"Ji-Wei Wu","tahun_publikasi":"2011","masalah":"Efficient linear text \r\nsegmentation","deskripsi_masalah":"Efficient linear text \r\nsegmentation algorithm based on hierarchical agglomerative \r\nclustering","keyword":"Text Segmentation","domain_data":"Test corpus","deskripsi_domain_data":"Test corpus consists of 700 samples. A \r\nsample is a concatenation of ten text segments. The 700 samples are divided into 4 sets according to the range of the number of sentences","metode":"Hierarchical learning strategy","deskripsi_metode":"Tokenization, stopword removal, \r\nand stemming. After text preprocessing, the text can be represented \r\nas vectors, each of which represents a sentence within the \r\ntext. A part of sentence similarities are then computed to \r\nconstruct the sentence-similarity matrix. Finally, the optimal \r\ntopic boundaries are identified by the proposed algorithm. ","hasil":"Linear text segmentation \r\nalgorithm (i.e., TSHAC) outperforms the linear time algorithm, TextTiling. TSHAC also provides comparable results with other algorithms. TSHAC provides a fully automatic process for linear text segmentation without auxiliary knowledge base, parameter setting, or user involvement.","creater":""}],"size":[1]},{"keyword":["Feature Selection"],"id":["10"],"sumbu_x":"Chinese text classification corpus","sumbu_y":"2005","children":[{"id":"10","judul":"A New Approach to Feature Selection in Text Classification","peneliti":"Yi Wang, Xiao-Jing Wang","tahun_publikasi":"2005","masalah":"New approach to feature selection to do feature reduction","deskripsi_masalah":"New approach to feature selection to do feature reduction, which is a constituent process in representing texts.","keyword":"Feature Selection","domain_data":"Chinese text classification corpus","deskripsi_domain_data":"Divide the corpus into two non-intersected sets: a training set containing 10 categories with 100 texts in each and a test set containing the same 10 categories with another 100 texts in each also","metode":"Variance-mean based feature filtering","deskripsi_metode":"Variance-mean based feature filtering method of feature selection to do feature reduction in the representation phase.","hasil":"Variance-mean method can gain higher performance at a very low dimension, and quickly reach a peak, which means much less computing time and almost best performance than DF, CHI.","creater":""}],"size":[1]},{"keyword":["Text Clustering"],"id":["11"],"sumbu_x":"Five test data sets(CACM, MED, EXC, PEO and TOP)","sumbu_y":"2008","children":[{"id":"11","judul":"Text Clustering with Feature Selection by Using Statistical Data","peneliti":"Yanjun Li, Congnan Luo, Soon M. Chung","tahun_publikasi":"2008","masalah":"Extended the X2 term-category indepen-\r\ndence test","deskripsi_masalah":"Extended the X2 term-category independence test by introducing new statistical data that can measure whether the dependency between a term and a category is positive or negative, developed a new supervised feature selection method, named CHIR, which is based on the X2 statistic and the new term-category dependency measure.","keyword":"Text Clustering","domain_data":"Five test data sets(CACM, MED, EXC, PEO and TOP)","deskripsi_domain_data":"Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category sets\r\nof the Reuters-21578 Distribution 1.0","metode":"TCFS","deskripsi_metode":"Text Clustering with Feature Selection (TCFS), which performs the clustering and the supervised feature selection alternately until convergence.","hasil":"CHIR consistently out-performs other three methods in terms of increasing the cohesiveness values of the clusters.","creater":""}],"size":[1]},{"keyword":["Clustering Feature Selection"],"id":["12"],"sumbu_x":"Gisette, Optdigits, covtype, hyperspectral image","sumbu_y":"2009","children":[{"id":"12","judul":"Clustering-Based Feature Selection in Semi-supervised Problems","peneliti":"Ianisse Quinz\u00e1n, Jos\u00e9 M. Sotoca, Filiberto Pla ","tahun_publikasi":"2009","masalah":"Unlabeled information can improve significant classification result","deskripsi_masalah":"Unlabeled information can improve significant classification result","keyword":"Clustering Feature Selection","domain_data":"Gisette, Optdigits, covtype, hyperspectral image","deskripsi_domain_data":"Gisette is a big data in the UCI repository, with 5000 attributes and 13500 objects, 7000 of them labelled. Optdigits problem is about the recognition of a handwritten number. The database has 5620 samples and 64 features.Covtype database, the objective is predicting forest \r\ncover type from cartographic variables, with no remotely sensed data. This database has 54 features, 581012 objects and 7 classes. A hyperspectral image called 92AV3C corresponding to a spectral image (145 x 145 pixels, 220 bands, and 17 classes).","metode":"Hybrid method (combines supervised and \r\nunsupervised measures of information)","deskripsi_metode":"A new hybrid method for semi-supervised \r\nproblem which combines supervised and unsupervised measures of information. This approach applies a strategy to obtain a feature subset through clustering techniques.","hasil":"The unsupervised information improves the accuracy and the ssfc method is adequate.\r\nOptdigits is a database where sup technique gets high-quality features for few labeled samples. Thus, in this case \r\nthe ssfc has similar performance than sup. Nevertheless when the number of labeled samples is increased, ssfc and sup become similar to supT. ","creater":""}],"size":[1]},{"keyword":["Discrete Particle Swarm","Constituent Dependencies","Background Knowledge"],"id":["13","16","17"],"sumbu_x":"GCE-2004 dataset","sumbu_y":"2010","children":[{"id":"13","judul":"A Discrete Particle Swarm Optimization Algorithm for Domain Independent Linear Text Segmentation","peneliti":"Ji-Wei Wu, Judy C.R. Tseng, Wen-Nung Tsai ","tahun_publikasi":"2010","masalah":"Improve the performance of linear text segmentation","deskripsi_masalah":"Improve the performance of linear text segmentation","keyword":"Discrete Particle Swarm","domain_data":"GCE-2004 dataset","deskripsi_domain_data":"Choi test corpus consists of 700 samples. A sample is a concatenation of ten text segments and each segment is the first in sentences of a randomly selected document from the Brown corpus.","metode":"DPSO-SEG","deskripsi_metode":"The goal of DPSO-SEG is to identify the optimal topic boundaries of the text segments in a document.  At first, the \r\nterms within each sentence are tokenized and stemmed. Then, generic stop words are removed.  After the basic \r\npreprocessing, each sentence is represented as a term-frequency vector. Then, sentence-sentence similarity \r\nbetween a pair of sentences is computed by cosine similarity. A sentence similarity matrix of the text then constructed using the sentence-sentence similarity. Finally, the optimal \r\nboundaries are created by DPSO according to the sentence similarity matrix. ","hasil":"The value of Pk is reduced sharply with fewer numbers of iterations, and smoothly after 350 iterations. It is converged at about 1500 iterations. the performance of DPSO-SEGC99 is better than DPSO-SEG. DPSO-SEGC99 also converges faster.","creater":""},{"id":"16","judul":"Exploiting Constituent Dependencies for Tree Kernel-Based Semantic Relation Extraction","peneliti":"Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian ","tahun_publikasi":"2010","masalah":"Dynamically determine the tree span for relation extraction by exploiting constituent dependencies","deskripsi_masalah":"Dynamically determine the tree span for relation extraction by exploiting constituent dependencies to integrate dependency information, which has been proven very useful to relation extraction, with the structured syntactic information to construct a concise and effective tree span specifically targeted for relation extraction. Explore interesting combined entity features for relation extraction via a unified parse and semantic tree. ","keyword":"Constituent Dependencies","domain_data":"GCE-2004 dataset","deskripsi_domain_data":"ACE RDC 2004 corpus as the benchmark data that contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes","metode":"Condense NounPhrases (NPs)\r\n","deskripsi_metode":"(1) Modification within base-NPs \r\n(2) Modification to NPs\r\n(3)Arguments\/adjuncts to verbs\r\n(4)Coordination conjunctions\r\n(5)Modification to other constituents","hasil":"the improvements of different tree setups over SPT. DSPT performs best among DSPT, SPT, CS-SPT. It also shows that the Unified Parse and Semantic Tree with Feature-Paired Tree perform significantly better than the other two tree setups (i.e., CS-SPT and DSPT).","creater":""},{"id":"17","judul":"Exploiting Background Knowledge for Relation Extraction","peneliti":"Yee Seng Chan and Dan Roth","tahun_publikasi":"2010","masalah":"Supervised RE","deskripsi_masalah":"Improve the performance of RE by considering the relationship between our relations of interest, as well as how they relate to some existing knowledge resources","keyword":"Background Knowledge","domain_data":"GCE-2004 dataset","deskripsi_domain_data":"ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments. ACE-2004 defines 7 coarse-grained relations and 23 fine-grained relations","metode":"Coarse-grained predictions","deskripsi_metode":"Using the coarse-grained predictions which should intuitively be more reliable, to improve the fine-grained predictions.Using Novel to contrain the predictions of the fine-grained.","hasil":"Performing the usual evaluation on mentions gives similar performance figures. All the background knowledge helped to improve performance, providing a total improvement of 3.9 to our basic RE system. Improves the performance of coarse-grained relation predictions.","creater":""}],"size":[1,2]},{"keyword":["First Order Statistics"],"id":["14"],"sumbu_x":"DNA microarray","sumbu_y":"2012","children":[{"id":"14","judul":"First Order Statistics Based Feature Selection: A Diverse and Powerful Family of Feature Seleciton Techniques","peneliti":"Taghi Khoshgoftaar, David Dittman, Randall Wald, and Alireza Fazelpour","tahun_publikasi":"2012","masalah":"First Order Statistics (FOS) based feature selection","deskripsi_masalah":"First Order Statistics (FOS) based feature selection using seven related univariate\r\nfeature selection metrics","keyword":"First Order Statistics","domain_data":"DNA microarray","deskripsi_domain_data":"The datasets are all DNA microarray datasets acquired from a number of different real world bioinformatics, genetics, and medical projects. Use datasets with two classes for example:\r\ncancerous\/non-cancerous or relapse\/no relapse). ","metode":"Datasets, feature subset size, similarity measure, and classification","deskripsi_metode":"Datasets, feature subset size, similarity measure, and classification","hasil":"Twenty one possible pairwise comparisons only one combination is above a 0.7 similarity across all twelve feature subset sizes: Fold Change Difference and SAM. Outside of this pair only four other pairs (S2N and Welch T Statistic, Signal to Noise and SAM, Fold Change Difference and Fisher Score, and Welch T Statistic and SAM) achieve a similarity score above 0.7 and only the combination of Welch T Statistic and Fisher Score achieves this below a feature subset size of 500","creater":""}],"size":[1]},{"keyword":["Relation Extraction"],"id":["15"],"sumbu_x":"Synonym dictionary for genes\/proteins","sumbu_y":"2007","children":[{"id":"15","judul":"Relation extraction using dependency parse trees","peneliti":"Katrin Fundel, Robert Ku\u00a8ffner, Ralf Zimmer","tahun_publikasi":"2007","masalah":"Relation extraction from free text","deskripsi_masalah":"The use of dependency parse trees as a means for biomedical relation extraction from free text. It is based on natural language preprocessing producing dependency parse trees and applying a small number of simple rules to these trees. ","keyword":"Relation Extraction","domain_data":"Synonym dictionary for genes\/proteins","deskripsi_domain_data":"Synonym dictionary for genes\/proteins, a training set (55 sentences and 103 interactions) and a test set (80 sentences and 54 interactions).","metode":"Effector-relation-effectee, relation-of-effectee-by-effector, relation-between-effector-and-effectee","deskripsi_metode":"(1) effector-relation-effectee (\u2018A activates B\u2019)\r\n(2) relation-of-effectee-by-effector (\u2018Activation of A by B\u2019)\r\n(3) relation-between-effector-and-effectee (\u2018Interaction between A\r\nand B\u2019).","hasil":"HPRD, even though being a very large\r\nand valuable source for protein interaction data, currently covers\r\nonly a small part of the human protein-protein relations from very limited relation categories. RelEx provides complementary information.","creater":""}],"size":[1]},{"keyword":["Automatic Evaluation"],"id":["18"],"sumbu_x":"ReVerb and SONEX","sumbu_y":"2012","children":[{"id":"18","judul":"Automatic Evaluation of Relation Extraction Systems on Large-scale","peneliti":"Mirko Bronzi, Zhaochen Guo, Filipe Mesquita","tahun_publikasi":"2012","masalah":"Framework for large-scale evaluation of relation extraction systems","deskripsi_masalah":"Framework for large-scale\r\nevaluation of relation extraction systems based on an automatic annotator that uses a public online database and a large web corpus.","keyword":"Automatic Evaluation","domain_data":"ReVerb and SONEX","deskripsi_domain_data":"Compare two open RE systems: ReVerb and SONEX. The input corpus for this comparison is the New York Times corpus, composed by 1.8 million documents. ReVerb  extracts relational phrases using rules over part-of-speech tags and noun-phrase chunks.","metode":"Automatic annotator","deskripsi_metode":"Use of an automatic annotator: a system capable of verifying whether or not a fact was correctly extracted. This is done by leveraging external sources of data and text, which are not available to the systems being evaluated","hasil":"About 63 million facts in G', the superset of the ground truth G. ","creater":""}],"size":[1]},{"keyword":["Partially Supervised Relation Extraction"],"id":["19"],"sumbu_x":"Three relations extracted","sumbu_y":"2006","children":[{"id":"19","judul":"Confidence Estimation Methods for Partially Supervised Relation Extraction","peneliti":"Eugene Agichtein","tahun_publikasi":"2006","masalah":"Extract structured relations between named entities","deskripsi_masalah":"Extract structured relations between named entities (e.g., a company name, a location name, or a name of a drug or a disease) from unstructured documents with minimal human effort. ","keyword":"Partially Supervised Relation Extraction","domain_data":"Three relations extracted","deskripsi_domain_data":"Three relations extracted from a collection of 145,000 articles from the New York Times from 1996, available as part of the North American News Text Corpus1.","metode":"Expectation Maximization (EM)","deskripsi_metode":"Expectation Maximization (EM) algorithms for estimating pattern and tuple confidence.","hasil":"The EM-based methods have higher accuracy than the constraint-based method","creater":""}],"size":[1]},{"keyword":["Information Extraction"],"id":["54"],"sumbu_x":"Berlin Hauptbahnho","sumbu_y":"2008","children":[{"id":"54","judul":"Unsupervised Relation Extraction from Web Documents","peneliti":"Kathrin Eichler, Holmer Hemsen and Guanter Neumann","tahun_publikasi":"2008","masalah":"Information extraction systems and technology","deskripsi_masalah":"Currently, IE systems are usually domain-dependent and adapting the system to a new domain requires a high amount of manual labour, such as specifying and implementing relation\u00e2\u20ac\u201cspecific extraction patterns manually or annotating large amounts of training corpora. Consequently, current IE technology is highly statically and inflexible with respect to a timely adap tation to new requirements in form of new topics.","keyword":"Information Extraction","domain_data":"Berlin Hauptbahnho","deskripsi_domain_data":"For our experiments, we built a test corpus of documents related to the topic Berlin Hauptbahnho by sending queries describing the topic (e.g., Berlin Hauptbahnho, Berlin central station) to Google and downloading the retrieved documents specifying English as the target language. After preprocessing these documents as described in 2.1., our corpus con sisted of 55,255 sentences from 1,068 web pages, from which 10773 relations were automatically extracted and clustered","metode":"Language guesser tool, LingPipe, named entity recognition, coreference resolution","deskripsi_metode":"In order to restrict the processing to sentences written in English, we apply a language guesser tool, lc4j (Lc4j, 2007) and remove sentences not classified as written in English. To all remaining sentences, we apply LingPipe (LingPipe, 2007) for sentence boundary detection, named entity recognition (NER) and coreference resolution.","hasil":"From the extracted relations, the system built 306 clusters of two or more instances, which were manually evaluated by two authors of this paper. 81 of our clusters contain two or more instances of exactly the same relation, mostly due to the same sentence appearing in several documents of the corpus. Of the remaining 225 clusters, 121 were marked as consistent (i.e., all instances in the cluster express a similar relation), 35 as partly consistent (i.e., more than half of the instances in the cluster express a similar relation), 69 as not use- ful.","creater":""}],"size":[1]},{"keyword":["Supervised, feature selection"],"id":["55"],"sumbu_x":"CACM data set","sumbu_y":"2008","children":[{"id":"55","judul":"Text Clustering with Feature Selection by Using Statistical Data","peneliti":"Yanjun Li, Congnan Luo, and Soon M. Chung","tahun_publikasi":"2008","masalah":"Supervised feature selection ","deskripsi_masalah":"Supervised feature selection methods using the information gain and the \u00cf\u20212 statistic can improve the clustering performance better than unsupervised methods when the class labels of documents are available for the feature selection. However, supervised feature selection methods cannot be directly applied to document clustering because usually the required class label information is not available.","keyword":"Supervised, feature selection","domain_data":"CACM data set","deskripsi_domain_data":"CACM data set","metode":"CHIR, TCFS","deskripsi_metode":"We also developed a new supervised feature selection method, named CHIR, which is based on the \u00cf\u20212 statistic and the new term category dependency measure. Unlike CHI, CHIR selects features having strong positive dependency on the categories. Furthermore, we explored CHIR in text clustering, and developed a new text clustering algorithm, named TCFS, which stands for Text Clustering with Feature Selection. Unlike the IF method, which performs text clustering and feature selection separately, TCFS integrates a supervised feature selection method, such as CHIR, into the text clustering process.","hasil":"Feature selection methods can improve the performance of text clustering as more irrelevant or redundant terms are removed. TCFS with a supervised feature selection method, such as CHIR, CHI or CC, can achieve a better F-measure than k-means with TS.","creater":""}],"size":[1]},{"keyword":["Cross-language, Query Translation"],"id":["56"],"sumbu_x":"Wikipedia articles","sumbu_y":"2008","children":[{"id":"56","judul":"WikiTranslate: Query Translation for Cross-Lingual Information Retrieval Using Only Wikipedia ","peneliti":"Dong Nguyen, Arnold Overwijk, Claudia Hauff, Dolf R.B. Trieschnigg, Djoerd Hiemstra, and Franciska M.G. de Jong ","tahun_publikasi":"2008","masalah":"Cross-lingual information retrieval using Wikipedia","deskripsi_masalah":"The aim of this research is to explore the possibilities of Wikipedia for query translation in CLIR. The main research question of this paper is: Is Wikipedia a viable alternative to current translation resources in cross-lingual information retrieval? This raises the following sub questions: How can queries be mapped to Wikipedia concepts? and how to create a query given the Wikipedia concepts?","keyword":"Cross-language, Query Translation","domain_data":"Wikipedia articles","deskripsi_domain_data":"Lucene is used as the underlying retrieval system to retrieve Wikipedia articles. From each article the title, text and cross-lingual links are extracted. The first paragraph of an article is extracted as well, which is called description. ","metode":"WikiTranslate, mapping, creating query","deskripsi_metode":"This paper presents WikiTranslate, a system which performs query  translation for cross-lingual information retrieval (CLIR) using only Wikipedia to obtain translations. We treat Wikipedia articles as representations of concepts (i.e. units of knowledge). The approach used by WikiTranslate consists of two important steps: mapping the query in source language to Wikipedia concepts and creating the final query in the target language using these found concepts.","hasil":"The system achieved a performance of 67% compared to the monolingual baseline. ","creater":""}],"size":[1]},{"keyword":["Text Segmentation, Subtopical Structure","DNA microarray datasets","Cross-language, Query Translation","Cross-language, Query Translation","Cross-language Information Retrieval, Machine Translation, Context"],"id":["67","68","69","70","71"],"sumbu_x":"Cross-linked Information Resources (CLIR)","sumbu_y":"2012","children":[{"id":"67","judul":"TopicTiling: A Text Segmentation Algorithm based on LDA","peneliti":"Martin Riedl and Chris Biemann","tahun_publikasi":"2012","masalah":"Text Segmentation","deskripsi_masalah":"The task tackled in this paper is Text Segmentation (TS), which is to be understood as the segmentation of texts into topically similar units. The challenge for a text segmentation algorithm is to find the subtopical structure of a text.","keyword":"Text Segmentation, Subtopical Structure","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"TextTiling, Latent Dirichlet Allocation","deskripsi_metode":"This algorithm is based on the well-known TextTiling algorithm, and segments documents using the Latent Dirichlet Allocation (LDA) topic model. TopicTiling uses topic IDs, obtained by the LDA inference method, instead of words. We denote this most probable topic ID as the mode (most frequent across all inference steps) of the topic assignment. These IDs are used to calculate the cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID.","hasil":"We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show significant improvements over state of the art segmentation algorithms on two standard datasets. As an additional benefit, TopicTiling performs the segmentation in lin- ear time and thus is computationally less expensive than other LDA-based segmentation methods.","creater":""},{"id":"68","judul":"First Order Statistics Based Feature Selection: A Diverse and Powerful Family of Feature Seleciton Techniques","peneliti":"Taghi Khoshgoftaar, David Dittman, Randall Wald, and Alireza Fazelpour","tahun_publikasi":"2012","masalah":"Reduction of the dimensionality of a dataset","deskripsi_masalah":"One of the most prevalent problems in DNA microarray datasets is the large degree of high dimensionality that is inherent in the data. Feature selection refers to a diverse series of techniques from the field of data mining designed for the reduction of the dimensionality of a dataset. However, there are a number of feature selection techniques which are computationally infeasible due to the severe level of high dimensionality found in DNA microarray datasets.","keyword":"DNA microarray datasets","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"First Order Statistics (FOS)","deskripsi_metode":"In order to examine the properties of these seven techniques we performed a series of similarity and classification experiments on eleven DNA microarray datasets. This paper presents a set of seven univariate feature selection techniques which we have combined into a family of techniques we name First Order Statistics (FOS) based feature selection.","hasil":"In terms of similarity, we find that each ranker in the FOS family of techniques will create diverse feature subsets when compared to other members of the family","creater":""},{"id":"69","judul":"Accurate Query Translation for Japanese-English Cross-language Information Retrieval","peneliti":"Vitaly Klyuev and Yannis Haralambous","tahun_publikasi":"2012","masalah":"Queries translation","deskripsi_masalah":"Cross-Language Information Retrieval (CLIR) can  be used to retrieve documents in one language in response to a query given in another. The usual approach consists of two steps: 1) translation of the user query into the target language and then 2) retrieval of documents in this language by using a conventional mono-lingual information retrieval system. In this paper, we propose a novel approach to translate queries for a Japanese-English CLIR task.","keyword":"Cross-language, Query Translation","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"EWC semantic relatedness, Wikipedia-based Explicit Semantic Analysis measure","deskripsi_metode":"To get all possible English senses for every Japanese term, the online dictionary SPACEALC is utilized. The EWC semantic relatedness measure is used to select the most related meanings for the results of translation. This measure combines the Wikipedia-based Explicit Semantic Analysis measure, the WordNet path measure and the mixed collocation index.","hasil":"Retrieval performance with queries generated utilizing Mecab was very low. Our preliminary experiments showed the superiority of the longest much technique applying SPACEALC over Mecab: Segmentation of Japanese texts is much more accurate.","creater":""},{"id":"70","judul":"Query Translation Using Concepts Similarity based on Quran Ontology for Cross-language Information Retrieval","peneliti":"Zulaini Yahya, Muhamad Taufik Abdullah, Azreen Azman and Rabiah Abdul Kadir ","tahun_publikasi":"2012","masalah":"Multi meaning words","deskripsi_masalah":"In Cross-Language Information Retrieval (CLIR) process, the translation effects have a direct impact on the accuracy of follow-up retrieval results. In dictionary-based approach, we are dealing with the words that have more than one meaning which can decrease the retrieval performance if the query translation return an incorrect translations.","keyword":"Cross-language, Query Translation","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"Domain ontology using Quran concepts","deskripsi_metode":"In this study we proposed a Cross-Language Information Retrieval (CLIR) method based on domain ontology using Quran concepts for disambiguating translation of the query and to improve the dictionary-based query translation.","hasil":"The experimental result shows that our proposed method brings significant improvement in retrieval accuracy for English document collections, but deficient for Malay document collections.","creater":""},{"id":"71","judul":"Combining Statistical Translation Techniques for Cross-Language Information Retrieval","peneliti":"Ferhan Ture1, Jimmy Lin, Douglas W. Oard","tahun_publikasi":"2012","masalah":"The use of old model of statistical machine translation systems","deskripsi_masalah":"Cross-language information retrieval today is dominated by techniques that rely principally on context-independent token-to-token mappings despite the fact that state-of-the-art statistical machine translation systems now have far richer translation models available in their internal representations.","keyword":"Cross-language Information Retrieval, Machine Translation, Context","domain_data":"Cross-linked Information Resources (CLIR)","deskripsi_domain_data":"CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.","metode":"Three types of statistical translation models","deskripsi_metode":"This paper explores combination-of-evidence techniques using three types of statistical translation models: context-independent token translation, token translation using phrase-dependent contexts, and token translation using sentence-dependent contexts. Context-independent translation is performed using statistically-aligned tokens in parallel text, phrase-dependent translation is performed using aligned statistical phrases, and sentence-dependent translation is performed using those same aligned phrases together with an n-gram language model.","hasil":"Experiments on retrieval of Arabic, Chinese, and French documents using English queries show that no one technique is optimal for all queries, but that statistically significant improvements in mean average precision over strong baselines can be achieved by combining translation evidence from all three techniques.","creater":""}],"size":[3,1,1]}],"links":[{"source":"1","target":"2","value":1},{"source":"2","target":"1","value":1},{"source":"2","target":"6","value":1},{"source":"2","target":"20","value":1},{"source":"10","target":"11","value":1},{"source":"12","target":"4","value":1},{"source":"11","target":"10","value":1},{"source":"2","target":"11","value":1},{"source":"8","target":"12","value":1}]}